import os
from time import sleep
import requests
from datetime import datetime, timedelta, timezone
from openai import OpenAI
from bs4 import BeautifulSoup
import pytz
from collections import Counter

# Language-related configurations
LANGUAGE_SETTINGS = {
    "zh": {
        "title": "Product Hunt ä»Šæ—¥çƒ­æ¦œ",
        "file_path": "content/zh-cn/post",
        "translate_task": "å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡,åªè¿”å›žè¯‘æ–‡:",
        "category_mapping": {
            "äººå·¥æ™ºèƒ½": ["AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "],
            "å·¥å…·": ["å·¥å…·", "ç”Ÿäº§åŠ›å·¥å…·", "æ•ˆçŽ‡", "Notion"],
            "å¼€å‘": ["API", "æ•°æ®åº“", "REST API", "SQL"]
        },
        "time_format": '%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)',
        "timezone": 'Asia/Shanghai',
        "tagline_label": "æ ‡è¯­",
        "description_label": "ä»‹ç»",
        "website_label": "ç½‘ç«™",
        "website_label2": "ç«‹å³è®¿é—®",
        "product_hunt_label": "åœ¨Product HuntæŸ¥çœ‹",
        "keyword_label": "å…³é”®è¯",
        "votes_count_label": "ç¥¨æ•°",
        "featured_label": "æ˜¯å¦ç²¾é€‰",
        "featured_mapping":  {"yes": "æ˜¯", "no": "å¦"},
        "created_at_label": "å‘å¸ƒæ—¶é—´",
        "keyword_prompt": "æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆé€‚åˆçš„ç®€çŸ­ä¸­æ–‡å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{name}\n\næ ‡è¯­ï¼š{tagline}\n\næè¿°ï¼š{description}",
        "default_keyword": "æ— å…³é”®è¯",
    },
    "en": {
        "title": "Product Hunt Daily",
        "file_path": "content/en/post",
        "translate_task": "Translate the following text to English,only return the translation:",
        "category_mapping": {
            "AI": ["AI", "Artificial intelligence", "machine learning", "AI agent"],
            "Tools": ["tool", "æ•ˆçŽ‡"],
            "Develop": ["API", "Database", "REST API", "SQL"]
        },
        "time_format": '%Y-%m-%d %I:%M %p (UTC)',
        "timezone": 'UTC',
        "tagline_label": "Tagline",
        "description_label": "Description",
        "website_label": "Website",
        "website_label2": "open",
        "product_hunt_label": "View on Product Hunt",
        "keyword_label": "Keyword",
        "votes_count_label": "VotesCount",
        "featured_label": "Featured",
        "featured_mapping":  {"yes": "Yes", "no": "No"},
        "created_at_label": "CreatedAt",
        "keyword_prompt": "Generate suitable short keywords based on the product information provided, separated by commas.\n\nName: {name}\nTagline: {tagline}\nDescription: {description}",
        "default_keyword": "No keywords",
    },
    "es": {
        "title": "Tendencias de Hoy",
        "file_path": "content/es/post",
        "translate_task": "Translate the following text to Spanish,only return the translation:",
        "category_mapping": {
            "Inteligencia Artificial": ["AI", "Inteligencia Artificial", "aprendizaje automÃ¡tico"],
            "Herramientas": ["herramienta", "productividad", "eficiencia", "Notion"],
            "Desarrollo": ["API", "base de datos", "REST API", "SQL"]
        },
        "time_format": '%Y-%m-%d %I:%M %p (UTC)',
        "timezone": 'Europe/Madrid',
        "tagline_label": "Lema",
        "description_label": "DescripciÃ³n",
        "website_label": "Sitio web",
        "website_label2": "Visitar",
        "product_hunt_label": "Ver en Product Hunt",
        "keyword_label": "Palabras clave",
        "votes_count_label": "Votos",
        "featured_label": "Destacado",
        "featured_mapping": {"yes": "SÃ­", "no": "No"},
        "created_at_label": "Fecha de creaciÃ³n",
        "keyword_prompt": "Generate suitable short keywords using Spanish based on the product information provided, separated by commas.\n\nName: {name}\nTagline: {tagline}\nDescription: {description}",
        "default_keyword": "Sin palabras clave"
    },
    "ar": {
        "title": "Ø§Ù„Ø£ÙƒØ«Ø± Ø±ÙˆØ§Ø¬Ù‹Ø§ Ø§Ù„ÙŠÙˆÙ…",
        "file_path": "content/ar/post",
        "translate_task": "å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆé˜¿æ‹‰ä¼¯è¯­,åªè¿”å›žè¯‘æ–‡:",
        "category_mapping": {
            "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ": ["Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "AI", "ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©"],
            "Ø§Ù„Ø£Ø¯ÙˆØ§Øª": ["Ø£Ø¯Ø§Ø©", "Ø¥Ù†ØªØ§Ø¬ÙŠØ©", "ÙƒÙØ§Ø¡Ø©", "Notion"],
            "Ø§Ù„ØªØ·ÙˆÙŠØ±": ["API", "Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª", "REST API", "SQL"]
        },
        "time_format": '%Y-%m-%d %I:%M %p (UTC)',
        "timezone": 'Asia/Riyadh',
        "tagline_label": "Ø§Ù„Ø´Ø¹Ø§Ø±",
        "description_label": "Ø§Ù„ÙˆØµÙ",
        "website_label": "Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
        "website_label2": "Ø²ÙŠØ§Ø±Ø©",
        "product_hunt_label": "Ø¹Ø±Ø¶ Ø¹Ù„Ù‰ Product Hunt",
        "keyword_label": "Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©",
        "votes_count_label": "Ø¹Ø¯Ø¯ Ø§Ù„Ø£ØµÙˆØ§Øª",
        "featured_label": "Ù…Ù…ÙŠØ²",
        "featured_mapping": {"yes": "Ù†Ø¹Ù…", "no": "Ù„Ø§"},
        "created_at_label": "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡",
        "keyword_prompt": "æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆç®€çŸ­çš„é€‚åˆçš„é˜¿æ‹‰ä¼¯è¯­å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{name}\n\næ ‡è¯­ï¼š{tagline}\n\næè¿°ï¼š{description}",
        "default_keyword": "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©"
    }
}

# Configure OpenAI client
api_key = os.getenv('OPENAI_API_KEY')
base_url = os.getenv('OPENAI_BASE_URL')
client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)

producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')
top_num = 10
model = "gemini-1.5-flash" 
#model = "gpt-4o-mini" 
#model = "deepseek-ai/DeepSeek-V2.5" 
lang = os.getenv('LANGUAGE')

class Product:
    def __init__(self, language: str, **kwargs):
        self.language = language
        self.settings = LANGUAGE_SETTINGS[language]
        self.name = kwargs.get('name')
        self.tagline = kwargs.get('tagline')
        self.description = kwargs.get('description')
        self.votes_count = kwargs.get('votesCount')
        self.featuredAt = kwargs.get('featuredAt')
        self.createdAt = kwargs.get('createdAt')
        self.website = kwargs.get('website')
        self.url = kwargs.get('url')
        
        self.is_en = (language == 'en')
        self.featured = self.settings["featured_mapping"]["yes"] if self.featuredAt else self.settings["featured_mapping"]["no"]
        self.created_at = self.convert_to_local_time(self.createdAt)
        self.translated_tagline = self.tagline if self.is_en else self.translate_text(self.tagline)
        self.translated_description = self.description if self.is_en else self.translate_text(self.description)
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = self.generate_keywords()

    def fetch_og_image_url(self) -> str:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
        }
        response = requests.get(self.url,headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image:
                return og_image["content"]
        return ""

    def generate_keywords(self) -> str:
        lang_settings = self.settings

        # Use language-specific prompt for keyword generation
        prompt = lang_settings['keyword_prompt'].format(
            name=self.name, tagline=self.tagline, description=self.description
        )

        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": prompt}],
                max_tokens=100,
                temperature=0.7,
                timeout=600,
            )
            keywords = response.choices[0].message.content.strip()
            return ', '.join(keywords.split()) if ',' not in keywords else keywords
        except Exception as e:
            print(f"Error occurred during keyword generation: {e}")
            return lang_settings['default_keyword']

    def translate_text(self, text: str) -> str:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": self.settings['translate_task']}, {"role": "user", "content": text}],
                max_tokens=500,
                temperature=0.7,
                timeout=600,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error occurred during translation: {e}")
            return text

    def convert_to_local_time(self, utc_time_str: str) -> str:
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        local_tz = pytz.timezone(self.settings['timezone'])
        local_time = utc_time.replace(tzinfo=pytz.utc).astimezone(local_tz)
        return local_time.strftime(self.settings['time_format'])

    def to_markdown(self, rank: int) -> str:
        lang_settings = self.settings

        # Use language-specific keywords for markdown content
        markdown_labels = {
            "rank": f"{rank}. {self.name}",
            "tagline":  lang_settings["tagline_label"],
            "description": lang_settings["description_label"],
            "website": lang_settings["website_label"],
            "website2": lang_settings["website_label2"],
            "product_hunt": lang_settings["product_hunt_label"],
            "keyword":  lang_settings["keyword_label"],
            "votes_count": lang_settings["votes_count_label"],
            "featured": lang_settings["featured_label"],
            "created_at": lang_settings["created_at_label"],
        }
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        fields = [
            f"## {markdown_labels['rank']}",
            f"**{markdown_labels['tagline']}**: {self.translated_tagline}",
            f"**{markdown_labels['description']}**: {self.translated_description}",
            f"**{markdown_labels['website']}**: [{markdown_labels['website2']}]({self.website})",
            f"**Product Hunt**: [{markdown_labels['product_hunt']}]({self.url})",
            og_image_markdown,
            f"**{markdown_labels['keyword']}**: {self.keyword}",
            f"**{markdown_labels['votes_count']}**: ðŸ”º{self.votes_count}",
            f"**{markdown_labels['featured']}**: {self.featured}",
            f"**{markdown_labels['created_at']}**: {self.created_at}",
            "\n"
            "---"
        ]
        return "  \n".join(fields) + "  \n\n"

def get_producthunt_token():
    url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": producthunt_client_id,
        "client_secret": producthunt_client_secret,
        "grant_type": "client_credentials",
    }
    headers = {
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
    }
    response = requests.post(url, json=payload, headers=headers)
    print(response.text)
    response.raise_for_status()
    return response.json().get("access_token")

def fetch_product_hunt_data():
    token = get_producthunt_token()
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
    }
    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """
    all_posts = []
    has_next_page = True
    cursor = ""
    while has_next_page and len(all_posts) < top_num:
        query = base_query % (date_str, date_str, cursor)
        response = requests.post(url, headers=headers, json={"query": query})
        response.raise_for_status()
        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)
        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']
    # åªè¿›è¡Œä¸€æ¬¡æŽ’åº
    sorted_posts = sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:top_num]
    return sorted_posts

def extract_keywords(products):
    keywords = [keyword.strip() for product in products for keyword in product.keyword.split(',') if keyword.strip()]
    return keywords

def count_and_sort_keywords(keywords):
    keyword_counts = Counter(keywords)
    return keyword_counts.most_common()

def generate_markdown(products, date_str: str, language: str):
    today = datetime.now(timezone.utc)
    date_today = today.strftime('%Y-%m-%d')
    date = today.strftime('%Y-%m-%d %H:%M:%S%z')
    keywords = extract_keywords(products)
    top_keywords = count_and_sort_keywords(keywords)
    
    settings = LANGUAGE_SETTINGS[language]
    path = settings['file_path']
    category_mapping = settings['category_mapping']

    os.makedirs(path, exist_ok=True)
    markdown_content = f"---\ntitle: {settings['title']} | {date_today}\ndate: {date}\nimage: {products[0].og_image_url}\n"

    if top_keywords:
        hugo_keywords = ', '.join([f'"{keyword}"' for keyword, _ in top_keywords[:3]])
        markdown_content += f"tags: [{hugo_keywords}]\n"
        categories = set()
        keyword_set = set(keyword for keyword, _ in top_keywords)
        for category, keywords in category_mapping.items():
            if any(keyword in keyword_set for keyword in keywords):
                categories.add(category)
        if categories:
            formatted_categories = ', '.join(f'"{category}"' for category in sorted(categories))
            markdown_content += f'categories: [{formatted_categories}]\n'
    markdown_content += "---\n\n" + ''.join(product.to_markdown(rank) for rank, product in enumerate(products, 1))

    file_name = f"{path}/producthunt-daily-{date_str}.md"
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")

def main():
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    posts = fetch_product_hunt_data()
    products = []
    # https://ai.google.dev/pricing?hl=zh-cn#1_5flash , gemini-1.5-flash é™åˆ¶äº†15RPM(æ¯åˆ†é’Ÿ15ä¸ªè¯·æ±‚)
    # æ¯ä¸ªpostéœ€è¦3æ¬¡
    for post in posts:
        products.append(Product(language=lang, **post))
        sleep(15)  # æ¯5ç§’é’Ÿç”Ÿæˆä¸€ä¸ªProductå¯¹è±¡
    generate_markdown(products, date_str, language=lang)

def save_csv():
    return 

if __name__ == "__main__":
    main()
