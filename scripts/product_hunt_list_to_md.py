import os
# from dotenv import load_dotenv
import requests
from datetime import datetime, timedelta, timezone
from openai import OpenAI
from bs4 import BeautifulSoup
import pytz
from collections import Counter
# åŠ è½½ .env æ–‡ä»¶
# load_dotenv()

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
api_key=os.getenv('OPENAI_API_KEY')
base_url=os.getenv('OPENAI_BASE_URL')
if base_url:
    client = OpenAI(api_key=api_key, base_url=base_url)
else:
    client = OpenAI(api_key=api_key)

producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')
# åªå–å‰ 10 æ¡æ•°æ®
top_num=10

model = "gemini-1.5-flash" 
#model = "gpt-4o-mini" 
#model = "deepseek-ai/DeepSeek-V2.5" 

# Define category to keywords mapping
# Add more categories and corresponding keywords as needed
category_mapping_zh = {
    "äººå·¥æ™ºèƒ½": ["AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "],
    "å·¥å…·": ["å·¥å…·", "ç”Ÿäº§åŠ›å·¥å…·", "æ•ˆç‡","Notion"],
    "å¼€å‘": ["API","æ•°æ®åº“","REST API","SQL"]
}

category_mapping_en = {
    "AI": ["AI", "Artificial intelligence", "machine learning","AI agent"],
    "Tools": ["tool", "æ•ˆç‡"],
    "Develop": ["API","Database","REST API","SQL"]
}

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str,en: bool, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.en = en
        if self.en:
            self.featured = "Yes" if featuredAt else "No"
            self.created_at = createdAt
            self.translated_tagline = self.tagline
            self.translated_description = self.description
        else:
            self.created_at = self.convert_to_beijing_time(createdAt)
            self.featured = "æ˜¯" if featuredAt else "å¦"
            self.translated_tagline = self.translate_text(self.tagline)
            self.translated_description = self.translate_text(self.description)
        self.website = website
        self.url = url
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = self.generate_keywords()

    def fetch_og_image_url(self) -> str:
        """è·å–äº§å“çš„Open Graphå›¾ç‰‡URL"""
        response = requests.get(self.url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image:
                return og_image["content"]
        return ""

    def generate_keywords(self) -> str:
        """ç”Ÿæˆäº§å“çš„å…³é”®è¯ï¼Œæ˜¾ç¤ºåœ¨ä¸€è¡Œï¼Œç”¨é€—å·åˆ†éš”"""
        if self.en:
            prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆé€‚åˆçš„è‹±æ–‡å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{self.name}\n\næ ‡è¯­ï¼š{self.tagline}\n\næè¿°ï¼š{self.description}"
        else:
            prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆé€‚åˆçš„ä¸­æ–‡å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{self.name}\n\næ ‡è¯­ï¼š{self.tagline}\n\næè¿°ï¼š{self.description}"
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Generate suitable Chinese keywords based on the product information provided. The keywords should be separated by commas."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=50,
                temperature=0.7,
                timeout=600,
            )
            keywords = response.choices[0].message.content.strip()
            if ',' not in keywords:
                keywords = ', '.join(keywords.split())
            return keywords
        except Exception as e:
            print(f"Error occurred during keyword generation: {e}")
            return "æ— å…³é”®è¯"

    def translate_text(self, text: str) -> str:
        """ä½¿ç”¨OpenAIç¿»è¯‘æ–‡æœ¬å†…å®¹"""
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸–ç•Œä¸Šæœ€ä¸“ä¸šçš„ç¿»è¯‘å·¥å…·ï¼Œæ“…é•¿è‹±æ–‡å’Œä¸­æ–‡äº’è¯‘ã€‚ä½ æ˜¯ä¸€ä½ç²¾é€šè‹±æ–‡å’Œä¸­æ–‡çš„ä¸“ä¸šç¿»è¯‘ï¼Œå°¤å…¶æ“…é•¿å°†ITå…¬å¸é»‘è¯å’Œä¸“ä¸šè¯æ±‡ç¿»è¯‘æˆç®€æ´æ˜“æ‡‚çš„åœ°é“è¡¨è¾¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆåœ°é“çš„ä¸­æ–‡ï¼Œé£æ ¼ä¸ç§‘æ™®æ‚å¿—æˆ–æ—¥å¸¸å¯¹è¯ç›¸ä¼¼ã€‚"},
                    {"role": "user", "content": text},
                ],
                max_tokens=500,
                temperature=0.7,
                timeout=600,
            )

            translated_text = response.choices[0].message.content.strip()
            return translated_text
        except Exception as e:
            print(f"Error occurred during translation: {e}")
            return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        """å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´"""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        """è¿”å›äº§å“æ•°æ®çš„Markdownæ ¼å¼"""
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        if self.en:
            # hugo éœ€è¦åŒç©ºæ ¼+\næ‰æ˜¯æ¢è¡Œ
            return (
                f"## {rank}. {self.name}\n"
                f"**Tagline**ï¼š{self.translated_tagline}  \n"
                f"**Description**ï¼š{self.translated_description}  \n"
                f"**Website**: [open]({self.website})  \n"
                f"**Product Hunt**: [View on Product Hunt]({self.url})  \n\n"
                f"{og_image_markdown}  \n\n"
                f"**Keyword**ï¼š{self.keyword}  \n"
                f"**VotesCount**: ğŸ”º{self.votes_count}  \n"
                f"**Featured**ï¼š{self.featured}  \n"
                f"**CreatedAt**ï¼š{self.created_at}  \n\n"
                f"---  \n\n"
            )
        else:                
            return (
                f"## {rank}. {self.name}\n"
                f"**æ ‡è¯­**ï¼š{self.translated_tagline}  \n"
                f"**ä»‹ç»**ï¼š{self.translated_description}  \n"
                f"**äº§å“ç½‘ç«™**: [ç«‹å³è®¿é—®]({self.website})  \n"
                f"**Product Hunt**: [View on Product Hunt]({self.url})  \n\n"
                f"{og_image_markdown}  \n\n"
                f"**å…³é”®è¯**ï¼š{self.keyword}  \n"
                f"**ç¥¨æ•°**: ğŸ”º{self.votes_count}  \n"
                f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured}  \n"
                f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}  \n\n"
                f"---  \n\n"
            )

def get_producthunt_token(): 
    """é€šè¿‡ client_id å’Œ client_secret è·å– Product Hunt çš„ access_token"""
    url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": producthunt_client_id,
        "client_secret": producthunt_client_secret,
        "grant_type": "client_credentials",
    }

    headers = {
        "Content-Type": "application/json",
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code != 200:
        raise Exception(f"Failed to obtain access token: {response.status_code}, {response.text}")

    token = response.json().get("access_token")
    return token

def fetch_product_hunt_data():
    """ä»Product Huntè·å–å‰ä¸€å¤©çš„Top 30æ•°æ®"""
    token = get_producthunt_token()
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    headers = {"Authorization": f"Bearer {token}"}

    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < top_num:
        query = base_query % (date_str, date_str, cursor)
        response = requests.post(url, headers=headers, json={"query": query})

        if response.status_code != 200:
            raise Exception(f"Failed to fetch data from Product Hunt: {response.status_code}, {response.text}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # åªä¿ç•™å‰top_numä¸ªäº§å“
    return [Product(**{**post, 'en': False}) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:top_num]],[Product(**{**post, 'en': True}) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:top_num]]


# æå–å…³é”®å­—
def extract_keywords(products,en:bool):
    keywords=[]
    max_len =10
    if en:
        max_len=30
    for product in products:
        if product.keyword:
            
            val=[keyword.strip() for keyword in product.keyword.split(',') if keyword.strip() and len(keyword)<max_len]
            keywords.extend(val)    
    return keywords

def count_and_sort_keywords(keywords):
    # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„å‡ºç°æ¬¡æ•°
    keyword_counts = Counter(keywords)
    # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    sorted_keywords = keyword_counts.most_common()
    return sorted_keywords

def generate_markdown(products, date_str:str,en:bool):
    """ç”ŸæˆMarkdownå†…å®¹å¹¶ä¿å­˜åˆ°dataç›®å½•"""
    # è·å–ä»Šå¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    today = datetime.now(timezone.utc)
    date_today = today.strftime('%Y-%m-%d')
    date = today.strftime('%Y-%m-%d %H:%M:%S%z')
    
    # æå–å…³é”®è¯
    keywords = extract_keywords(products,en=en)
    top_keywords=count_and_sort_keywords(keywords)

    ## æ–‡ç« æ ‡é¢˜
    markdown_content="---\n"
    if en:
        markdown_content += f"title: Producthunt Daily | {date_today}\n"
        markdown_content += f"date: {date}\n"
        markdown_content += f"image: {products[0].og_image_url}\n"  
        category_mapping=category_mapping_en
    else:
        markdown_content += f"title: ä»Šæ—¥çƒ­æ¦œ | {date_today}\n"
        markdown_content += f"date: {date}\n"
        markdown_content += f"image: {products[0].og_image_url}\n"        
        category_mapping=category_mapping_zh
    
    if len(top_keywords)>0:
        hugo_keywords = ', '.join([f'"{keyword}"' for keyword, _ in top_keywords[:3]])
        markdown_content += f'tags: [{hugo_keywords}]\n'

        categories = set()
        keyword_set = set(keyword for keyword, _ in top_keywords)
        for category, keywords in category_mapping.items():
            if any(keyword in keyword_set for keyword in keywords):
                categories.add(category)    
        if categories:
            vals = list(categories)
            vals.sort()
            formatted_categories = ', '.join(f'"{category}"' for category in vals)
            markdown_content += f'categories: [{formatted_categories}]\n'      
    markdown_content+="---\n\n"
    
    ## å†…å®¹
    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    os.makedirs('content/en/post', exist_ok=True)
    os.makedirs('content/zh-cn/post', exist_ok=True)

    # ä¿®æ”¹æ–‡ä»¶ä¿å­˜è·¯å¾„åˆ° data ç›®å½•
    if en:
        file_name = f"content/en/post/producthunt-daily-{date_str}.md"
    else:
        file_name = f"content/zh-cn/post/producthunt-daily-{date_str}.md"
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")


def main():
    # è·å–æ˜¨å¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')

    # è·å–Product Huntæ•°æ®
    products,productsEn = fetch_product_hunt_data()

    # ç”ŸæˆMarkdownæ–‡ä»¶
    generate_markdown(products, date_str,en=False)
    generate_markdown(productsEn, date_str,en=True)

if __name__ == "__main__":
    main()