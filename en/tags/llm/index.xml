<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on AI News</title>
        <link>https://ai.programnotes.cn/en/tags/llm/</link>
        <description>Recent content in LLM on AI News</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Thu, 28 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://ai.programnotes.cn/en/tags/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>The Model Context Protocol (MCP): A Comprehensive Analysis</title>
        <link>https://ai.programnotes.cn/en/p/the-model-context-protocol-mcp-a-comprehensive-analysis/</link>
        <pubDate>Thu, 28 Aug 2025 00:00:00 +0000</pubDate>
        
        <guid>https://ai.programnotes.cn/en/p/the-model-context-protocol-mcp-a-comprehensive-analysis/</guid>
        <description>&lt;p&gt;The Model Context Protocol (MCP) is rapidly emerging as a pivotal open standard, poised to revolutionize how AI applications engage with external data sources, tools, and APIs. Conceived by Anthropic, MCP seeks to establish a harmonized and standardized interface, empowering Large Language Models (LLMs) and AI agents to harness external resources with unprecedented efficiency and security. This analysis provides a comprehensive examination of MCP, dissecting its architecture, advantages, limitations, adoption patterns, and its standing relative to alternative methodologies.[^1]&lt;/p&gt;
&lt;h2 id=&#34;core-architecture&#34;&gt;Core Architecture
&lt;/h2&gt;&lt;p&gt;MCP adopts a client-server architecture, wherein AI applications (clients) interface with MCP servers that serve as intermediaries to external resources. The protocol leverages JSON-RPC 2.0 and accommodates transports such as Server-Sent Events (SSE). Key operational facets include dynamic tool discovery, a standardized interface, context management, and robust security protocols.[^2]&lt;/p&gt;
&lt;h2 id=&#34;functionalitydescription&#34;&gt;Functionality	Description
&lt;/h2&gt;&lt;p&gt;Tool Discovery	Enables AI agents to dynamically identify available tools and their schemas, facilitating optimal tool selection for specific tasks.
Standardized Interface	Offers a unified interface for interacting with diverse APIs, tools, and data sources, thereby minimizing the need for custom code and simplifying integration processes.
Context Management	Streamlines context management across multiple interactions, ensuring AI agents possess the requisite information for executing intricate tasks.
Security	Integrates security measures to avert unauthorized access and safeguard sensitive data, maintaining data integrity and confidentiality.
Key Advantages
MCP presents several compelling advantages over conventional AI integration techniques, such as function calling and direct API invocations. These benefits include simplified integration processes, improved scalability, enhanced security measures, and dynamic tool discovery capabilities.&lt;/p&gt;
&lt;p&gt;Simplified integration is achieved through MCP&amp;rsquo;s standardized interface and tool discovery mechanism, which significantly reduces the complexities associated with integrating AI with external resources. This streamlined approach not only accelerates development cycles but also lowers the barrier to entry for developers seeking to leverage AI in their applications.&lt;/p&gt;
&lt;p&gt;Scalability is promoted by decoupling tool implementation from consumption, allowing AI agents to access a broad spectrum of tools without needing specific knowledge of their implementation nuances. This decoupling fosters a more flexible and scalable AI ecosystem, where new tools can be seamlessly integrated and utilized across various applications.&lt;/p&gt;
&lt;p&gt;Enhanced security is realized through centralized control and monitoring of AI interactions, which improves overall security and mitigates the risk of unauthorized access. MCP&amp;rsquo;s security architecture provides a robust framework for managing permissions and access controls, ensuring that sensitive data remains protected.&lt;/p&gt;
&lt;p&gt;Dynamic tool discovery empowers AI agents to discover and utilize new tools on-the-fly, enhancing their adaptability and versatility. This dynamic capability enables AI agents to respond to changing conditions and emerging opportunities, making them more effective in dynamic and unpredictable environments.&lt;/p&gt;
&lt;h2 id=&#34;inherent-limitations&#34;&gt;Inherent Limitations
&lt;/h2&gt;&lt;p&gt;Despite its promise, MCP is not without its limitations and challenges. These include concerns about its maturity as a relatively new technology, the need for widespread adoption, the complexity of implementing and managing MCP servers, potential security risks, and the overhead associated with running MCP clients within host applications.[^3]&lt;/p&gt;
&lt;p&gt;The immaturity of MCP as a technology means that its long-term viability remains uncertain. As the protocol evolves, it is essential to address any shortcomings and ensure that it meets the needs of the AI community.&lt;/p&gt;
&lt;p&gt;Widespread adoption of MCP hinges on support from major AI providers and developers, which may take time to materialize. Overcoming this adoption hurdle requires demonstrating the value of MCP and fostering a collaborative ecosystem where developers can contribute to its growth.&lt;/p&gt;
&lt;p&gt;Implementing and managing MCP servers can be a complex undertaking, requiring specialized expertise. Simplifying the deployment and management of MCP servers is crucial for lowering the barrier to entry and encouraging broader adoption.&lt;/p&gt;
&lt;p&gt;MCP introduces new security risks, such as prompt injection attacks and unauthorized access, which must be carefully addressed. Implementing robust security measures and adhering to best practices is essential for mitigating these risks and ensuring the integrity of AI systems.&lt;/p&gt;
&lt;p&gt;The overhead of running MCP clients within host applications can impact performance, particularly in resource-constrained environments. Optimizing the performance of MCP clients is essential for ensuring that AI applications remain responsive and efficient.&lt;/p&gt;
&lt;p&gt;Adoption Trends
MCP is steadily gaining traction within the AI community, with numerous companies and organizations actively exploring its potential. Early adopters, including Block (Square), Apollo, Zed, Replit, Codeium, and Sourcegraph, are pioneering the integration of MCP into diverse applications and platforms. These include Integrated Development Environments (IDEs), enterprise AI deployments, and agentic Retrieval-Augmented Generation (RAG) applications.[^4]&lt;/p&gt;
&lt;h2 id=&#34;comparative-analysis&#34;&gt;Comparative Analysis
&lt;/h2&gt;&lt;p&gt;MCP is frequently contrasted with function calling, a mechanism that enables LLMs to directly invoke predefined functions. While function calling facilitates AI model interaction with external resources, it lacks the standardization and scalability inherent in MCP. Furthermore, MCP diverges from other AI agent protocols, such as A2A, which emphasizes agent-to-agent communication rather than model-to-tool interactions.[^5]&lt;/p&gt;
&lt;p&gt;Function calling, while useful for specific tasks, often requires custom implementations for each LLM, leading to a fragmented and less scalable ecosystem. MCP, on the other hand, provides a unified interface that can be used across different LLMs, promoting interoperability and reducing the need for custom code.&lt;/p&gt;
&lt;p&gt;A2A protocols focus on enabling collaboration between AI agents, whereas MCP focuses on enabling AI agents to access and utilize external resources. While these protocols address different aspects of AI system design, they can be complementary, with MCP providing the infrastructure for accessing tools and A2A providing the framework for coordinating multi-agent interactions.&lt;/p&gt;
&lt;h2 id=&#34;future-implications&#34;&gt;Future Implications
&lt;/h2&gt;&lt;p&gt;MCP holds significant promise as a means of standardizing AI integration, offering substantial advantages over traditional methodologies. Despite existing challenges, the burgeoning interest in MCP suggests its potential to play a pivotal role in the future of AI. As the protocol matures and its adoption broadens, it is poised to unlock novel possibilities for AI-powered applications and services.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The future success of MCP will depend on addressing its limitations, fostering a collaborative ecosystem, and demonstrating its value to the broader AI community. By overcoming these challenges, MCP can pave the way for a more integrated, scalable, and secure AI landscape, where AI agents can seamlessly interact with external resources to solve complex problems and create new opportunities.&lt;/p&gt;
&lt;p&gt;The question remains: Can MCP truly become the &amp;ldquo;USB-C&amp;rdquo; of AI, or will it be relegated to a niche technology overshadowed by proprietary solutions? Only time, and the collective efforts of the AI community, will reveal the answer.&lt;/p&gt;
&lt;h2 id=&#34;ref&#34;&gt;Ref
&lt;/h2&gt;&lt;p&gt;[1]: MCP is promising but immature explore its security flaws cost issues and why orchestration and backward compatibility remain major hurdles MCP Will be the Death of Low-Code Automation, and Other&lt;/p&gt;
&lt;p&gt;[2]: The Model Context Protocol MCP is an open standard introduced by Anthropic with the goal to standardize how AI applications chatbots IDE assistants or Model Context Protocol (MCP) an overview - Philschmid&lt;/p&gt;
&lt;p&gt;[3]: Community and Adoption In just a few months MCP went from concept to a growing ecosystem Early adopters included companies like Block Square Apollo Zed Replit Codeium and Sourcegraph who began integrating MCP to enhance their platforms Fast forward to 2025 and the ecosystem has exploded by February there were over 1 000 community built MCP servers connectors available Clearly MCP has struck a chord as the industry moves toward more integrated and context aware AI This network effect makes MCP even more attractive the more tools available via MCP the more useful it is to adopt the standard What Is MCP, and Why Is Everyone – Suddenly!– Talking About It?&lt;/p&gt;
&lt;p&gt;[4]: MCP is rapidly maturing into a powerful standard protocol that turns AI from an isolated brain into a versatile doer By streamlining how agents connect with external systems it clears the path for more capable interactive and user friendly AI workflows What Is MCP, and Why Is Everyone – Suddenly!– Talking About It?&lt;/p&gt;
&lt;p&gt;[5]: Quick Comparison Function Calling vs MCP vs A2A It s tempting to see these protocols as competitors but they actually solve different If Function Calling is like having to speak multiple languages to different chefs MCP is like having a universal translator in the kitchen Define your tools In architectural terms MCP answers what tools can my agent use while A2A handles how can my agents work together This resembles how While Function Calling and MCP focus on model to tool interaction A2A Agent to Agent Protocol introduced by Google tackles a different The Great AI Agent Protocol Race: Function Calling vs. MCP vs. A2A&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;MCP follows a client host server architecture where each host can run multiple client instances This architecture enables users to integrate AI capabilities The Model Context Protocol (MCP) — A Complete Tutorial - Medium&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
        <item>
        <title>prompt-optimizer</title>
        <link>https://ai.programnotes.cn/en/p/prompt-optimizer/</link>
        <pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate>
        
        <guid>https://ai.programnotes.cn/en/p/prompt-optimizer/</guid>
        <description>&lt;img src="https://images.unsplash.com/photo-1648914300949-a59ba0614055?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE3NTAzMTgxMzV8&amp;ixlib=rb-4.1.0" alt="Featured image of post prompt-optimizer" /&gt;&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/README_EN.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;English&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/README.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;中文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/stargazers&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;img src=&#34;https://img.shields.io/github/stars/linshenkx/prompt-optimizer&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GitHub stars&#34;
	
	
&gt;&lt;/a&gt;
&lt;img src=&#34;https://img.shields.io/chrome-web-store/users/cakkkhboolfnadechdlgdcnjammejlna?style=flat&amp;amp;label=Chrome%20Users&amp;amp;link=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2F%25E6%258F%2590%25E7%25A4%25BA%25E8%25AF%258D%25E4%25BC%2598%25E5%258C%2596%25E5%2599%25A8%2Fcakkkhboolfnadechdlgdcnjammejlna&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Chrome Web Store Users&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/LICENSE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;License&#34;
	
	
&gt;&lt;/a&gt;
&lt;a class=&#34;link&#34; href=&#34;https://hub.docker.com/r/linshen/prompt-optimizer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/linshen/prompt-optimizer&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Docker Pulls&#34;
	
	
&gt;&lt;/a&gt;
&lt;img src=&#34;https://img.shields.io/github/forks/linshenkx/prompt-optimizer?style=flat&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GitHub forks&#34;
	
	
&gt;
&lt;a class=&#34;link&#34; href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flinshenkx%2Fprompt-optimizer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;img src=&#34;https://img.shields.io/badge/Vercel-indigo?style=flat&amp;amp;logo=vercel&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Deploy with Vercel&#34;
	
	
&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://prompt.always200.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Live Demo&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;#quick-start&#34; &gt;Quick Start&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;#faq&#34; &gt;FAQ&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/dev.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Development Docs&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/vercel_en.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vercel Deployment Guide&lt;/a&gt; | &lt;a class=&#34;link&#34; href=&#34;https://chromewebstore.google.com/detail/prompt-optimizer/cakkkhboolfnadechdlgdcnjammejlna&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Chrome Extension&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;-project-introduction&#34;&gt;📖 Project Introduction
&lt;/h2&gt;&lt;p&gt;Prompt Optimizer is a powerful AI prompt optimization tool that helps you write better AI prompts and improve the quality of AI outputs. It supports both web application and Chrome extension usage.&lt;/p&gt;
&lt;h3 id=&#34;-feature-demonstration&#34;&gt;🎥 Feature Demonstration
&lt;/h3&gt;&lt;div align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://ai.programnotes.cn/img/github/prompt-optimizer-contrast.png&#34; alt=&#34;Feature Demonstration&#34; width=&#34;90%&#34;&gt;
&lt;/div&gt;
&lt;h2 id=&#34;-core-features&#34;&gt;✨ Core Features
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;🎯 &lt;strong&gt;Intelligent Optimization&lt;/strong&gt;: One-click prompt optimization with multi-round iterative improvements to enhance AI response accuracy&lt;/li&gt;
&lt;li&gt;🔄 &lt;strong&gt;Comparison Testing&lt;/strong&gt;: Real-time comparison between original and optimized prompts for intuitive demonstration of optimization effects&lt;/li&gt;
&lt;li&gt;🤖 &lt;strong&gt;Multi-model Integration&lt;/strong&gt;: Support for mainstream AI models including OpenAI, Gemini, DeepSeek, Zhipu AI, SiliconFlow, etc.&lt;/li&gt;
&lt;li&gt;⚙️ &lt;strong&gt;Advanced Parameter Configuration&lt;/strong&gt;: Support for individual LLM parameter configuration (temperature, max_tokens, etc.) for each model&lt;/li&gt;
&lt;li&gt;🔒 &lt;strong&gt;Secure Architecture&lt;/strong&gt;: Pure client-side processing with direct data interaction with AI service providers, bypassing intermediate servers&lt;/li&gt;
&lt;li&gt;💾 &lt;strong&gt;Privacy Protection&lt;/strong&gt;: Local encrypted storage of history records and API keys with data import/export support&lt;/li&gt;
&lt;li&gt;📱 &lt;strong&gt;Multi-platform Support&lt;/strong&gt;: Available as both a web application and Chrome extension&lt;/li&gt;
&lt;li&gt;🎨 &lt;strong&gt;User Experience&lt;/strong&gt;: Clean and intuitive interface design with responsive layout and smooth interaction effects&lt;/li&gt;
&lt;li&gt;🌐 &lt;strong&gt;Cross-domain Support&lt;/strong&gt;: Edge Runtime proxy for cross-domain issues when deployed on Vercel&lt;/li&gt;
&lt;li&gt;🔐 &lt;strong&gt;Access Control&lt;/strong&gt;: Password protection feature for secure deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start
&lt;/h2&gt;&lt;h3 id=&#34;1-use-online-version-recommended&#34;&gt;1. Use Online Version (Recommended)
&lt;/h3&gt;&lt;p&gt;Direct access: &lt;a class=&#34;link&#34; href=&#34;https://prompt.always200.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://prompt.always200.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a pure frontend project with all data stored locally in your browser and never uploaded to any server, making the online version both safe and reliable to use.&lt;/p&gt;
&lt;h3 id=&#34;2-vercel-deployment&#34;&gt;2. Vercel Deployment
&lt;/h3&gt;&lt;p&gt;Method 1: One-click deployment to your own Vercel:
&lt;a class=&#34;link&#34; href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flinshenkx%2Fprompt-optimizer&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;img src=&#34;https://vercel.com/button&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Deploy with Vercel&#34;
	
	
&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Method 2: Fork the project and import to Vercel (Recommended):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First fork the project to your GitHub account&lt;/li&gt;
&lt;li&gt;Then import the project to Vercel&lt;/li&gt;
&lt;li&gt;This allows tracking of source project updates for easy syncing of new features and fixes&lt;/li&gt;
&lt;li&gt;Configure environment variables:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ACCESS_PASSWORD&lt;/code&gt;: Set access password to enable access restriction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VITE_OPENAI_API_KEY&lt;/code&gt; etc.: Configure API keys for various AI service providers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more detailed deployment steps and important notes, please check:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/vercel_en.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vercel Deployment Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-install-chrome-extension&#34;&gt;3. Install Chrome Extension
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Install from Chrome Web Store (may not be the latest version due to approval delays): &lt;a class=&#34;link&#34; href=&#34;https://chromewebstore.google.com/detail/prompt-optimizer/cakkkhboolfnadechdlgdcnjammejlna&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Chrome Web Store&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click the icon to open the Prompt Optimizer&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;4-docker-deployment&#34;&gt;4. Docker Deployment
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Run container (default configuration)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker run -d -p 80:80 --restart unless-stopped --name prompt-optimizer linshen/prompt-optimizer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Run container (with API key configuration and password protection)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker run -d -p 80:80 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -e &lt;span class=&#34;nv&#34;&gt;VITE_OPENAI_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -e &lt;span class=&#34;nv&#34;&gt;ACCESS_USERNAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_username &lt;span class=&#34;se&#34;&gt;\ &lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Optional, defaults to &amp;#34;admin&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  -e &lt;span class=&#34;nv&#34;&gt;ACCESS_PASSWORD&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_password &lt;span class=&#34;se&#34;&gt;\ &lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Required for password protection&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  --restart unless-stopped &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --name prompt-optimizer &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  linshen/prompt-optimizer
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;5-docker-compose-deployment&#34;&gt;5. Docker Compose Deployment
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Clone the repository&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/linshenkx/prompt-optimizer.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; prompt-optimizer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. Optional: Create .env file for API keys and authentication&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cat &amp;gt; .env &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;# API Key Configuration
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;VITE_OPENAI_API_KEY=your_openai_api_key
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;VITE_GEMINI_API_KEY=your_gemini_api_key
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;VITE_DEEPSEEK_API_KEY=your_deepseek_api_key
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;VITE_ZHIPU_API_KEY=your_zhipu_api_key
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;VITE_SILICONFLOW_API_KEY=your_siliconflow_api_key
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;# Basic Authentication
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;ACCESS_USERNAME=your_username  # Optional, defaults to &amp;#34;admin&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;ACCESS_PASSWORD=your_password  # Required for authentication
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. Start the service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker compose up -d
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4. View logs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;docker compose logs -f
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;You can also edit the docker-compose.yml file directly to customize your configuration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;services&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;prompt-optimizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;linshen/prompt-optimizer:latest&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;container_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;prompt-optimizer&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;restart&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;unless-stopped&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;8081:80&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Change port mapping&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;environment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;VITE_OPENAI_API_KEY=your_key_here &lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Set API key directly in config&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;-api-key-configuration&#34;&gt;⚙️ API Key Configuration
&lt;/h2&gt;&lt;h3 id=&#34;method-1-via-interface-recommended&#34;&gt;Method 1: Via Interface (Recommended)
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Click the &amp;ldquo;⚙️Settings&amp;rdquo; button in the upper right corner&lt;/li&gt;
&lt;li&gt;Select the &amp;ldquo;Model Management&amp;rdquo; tab&lt;/li&gt;
&lt;li&gt;Click on the model you need to configure (such as OpenAI, Gemini, DeepSeek, etc.)&lt;/li&gt;
&lt;li&gt;Enter the corresponding API key in the configuration box&lt;/li&gt;
&lt;li&gt;Click &amp;ldquo;Save&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Supported models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI (gpt-3.5-turbo, gpt-4, gpt-4o)&lt;/li&gt;
&lt;li&gt;Gemini (gemini-1.5-pro, gemini-2.0-flash)&lt;/li&gt;
&lt;li&gt;DeepSeek (deepseek-chat, deepseek-coder)&lt;/li&gt;
&lt;li&gt;Zhipu AI (glm-4-flash, glm-4, glm-3-turbo)&lt;/li&gt;
&lt;li&gt;SiliconFlow (Pro/deepseek-ai/DeepSeek-V3)&lt;/li&gt;
&lt;li&gt;Custom API (OpenAI compatible interface)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to API keys, you can configure advanced LLM parameters for each model individually. These parameters are configured through a field called &lt;code&gt;llmParams&lt;/code&gt;, which allows you to specify any parameters supported by the LLM SDK in key-value pairs for fine-grained control over model behavior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced LLM Parameter Configuration Examples:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenAI/Compatible APIs&lt;/strong&gt;: &lt;code&gt;{&amp;quot;temperature&amp;quot;: 0.7, &amp;quot;max_tokens&amp;quot;: 4096, &amp;quot;timeout&amp;quot;: 60000}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gemini&lt;/strong&gt;: &lt;code&gt;{&amp;quot;temperature&amp;quot;: 0.8, &amp;quot;maxOutputTokens&amp;quot;: 2048, &amp;quot;topP&amp;quot;: 0.95}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;{&amp;quot;temperature&amp;quot;: 0.5, &amp;quot;top_p&amp;quot;: 0.9, &amp;quot;frequency_penalty&amp;quot;: 0.1}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more detailed information about &lt;code&gt;llmParams&lt;/code&gt; configuration, please refer to the &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/llm-params-guide.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLM Parameters Configuration Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;method-2-via-environment-variables&#34;&gt;Method 2: Via Environment Variables
&lt;/h3&gt;&lt;p&gt;Configure environment variables through the &lt;code&gt;-e&lt;/code&gt; parameter when deploying with Docker:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_OPENAI_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_GEMINI_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_DEEPSEEK_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_ZHIPU_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_SILICONFLOW_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_CUSTOM_API_KEY&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_custom_api_key
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_CUSTOM_API_BASE_URL&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_custom_api_base_url
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-e &lt;span class=&#34;nv&#34;&gt;VITE_CUSTOM_API_MODEL&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your_custom_model_name
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;local-development&#34;&gt;Local Development
&lt;/h2&gt;&lt;p&gt;For detailed documentation, see &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/dev.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Development Documentation&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Clone the project&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/linshenkx/prompt-optimizer.git
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; prompt-optimizer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2. Install dependencies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm install
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 3. Start development server&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm dev               &lt;span class=&#34;c1&#34;&gt;# Main development command: build core/ui and run web app&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm dev:web          &lt;span class=&#34;c1&#34;&gt;# Run web app only&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pnpm dev:fresh        &lt;span class=&#34;c1&#34;&gt;# Complete reset and restart development environment&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;-roadmap&#34;&gt;🗺️ Roadmap
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Basic feature development&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Web application release&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Chrome extension release&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Custom model support&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Multi-model support optimization&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Internationalization support&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For detailed project status, see &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/project-status.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Project Status Document&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;-related-documentation&#34;&gt;📖 Related Documentation
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/README.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Documentation Index&lt;/a&gt; - Index of all documentation&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/technical-development-guide.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Technical Development Guide&lt;/a&gt; - Technology stack and development specifications&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/llm-params-guide.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLM Parameters Configuration Guide&lt;/a&gt; - Detailed guide for advanced LLM parameter configuration&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/project-structure.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Project Structure&lt;/a&gt; - Detailed project structure description&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/project-status.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Project Status&lt;/a&gt; - Current progress and plans&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/prd.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Product Requirements&lt;/a&gt; - Product requirements document&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/vercel_en.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vercel Deployment Guide&lt;/a&gt; - Detailed instructions for Vercel deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;star-history&#34;&gt;Star History
&lt;/h2&gt;&lt;a href=&#34;https://star-history.com/#linshenkx/prompt-optimizer&amp;Date&#34;&gt;
 &lt;picture&gt;
   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&amp;type=Date&amp;theme=dark&#34; /&gt;
   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&amp;type=Date&#34; /&gt;
   &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&amp;type=Date&#34; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
&lt;h2 id=&#34;faq&#34;&gt;FAQ
&lt;/h2&gt;&lt;h3 id=&#34;api-connection-issues&#34;&gt;API Connection Issues
&lt;/h3&gt;&lt;h4 id=&#34;q1-why-cant-i-connect-to-the-model-service-after-configuring-the-api-key&#34;&gt;Q1: Why can&amp;rsquo;t I connect to the model service after configuring the API key?
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Most connection failures are caused by &lt;strong&gt;Cross-Origin Resource Sharing (CORS)&lt;/strong&gt; issues. As this project is a pure frontend application, browsers block direct access to API services from different origins for security reasons. Model services will reject direct requests from browsers if CORS policies are not correctly configured.&lt;/p&gt;
&lt;h4 id=&#34;q2-how-to-solve-ollama-connection-issues&#34;&gt;Q2: How to solve Ollama connection issues?
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Ollama fully supports the OpenAI standard interface, just configure the correct CORS policy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_ORIGINS=*&lt;/code&gt; to allow requests from any origin&lt;/li&gt;
&lt;li&gt;If issues persist, set &lt;code&gt;OLLAMA_HOST=0.0.0.0:11434&lt;/code&gt; to listen on any IP address&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;q3-how-to-solve-cors-issues-with-commercial-apis-such-as-nvidias-ds-api-bytedances-volcano-api&#34;&gt;Q3: How to solve CORS issues with commercial APIs (such as Nvidia&amp;rsquo;s DS API, ByteDance&amp;rsquo;s Volcano API)?
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: These platforms typically have strict CORS restrictions. Recommended solutions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use Vercel Proxy&lt;/strong&gt; (Convenient solution)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the online version: &lt;a class=&#34;link&#34; href=&#34;https://prompt.always200.com&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;prompt.always200.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Or deploy to your own Vercel platform&lt;/li&gt;
&lt;li&gt;Check &amp;ldquo;Use Vercel Proxy&amp;rdquo; option in model settings&lt;/li&gt;
&lt;li&gt;Request flow: Browser → Vercel → Model service provider&lt;/li&gt;
&lt;li&gt;For detailed steps, please refer to the &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/docs/vercel_en.md&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vercel Deployment Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use self-deployed API proxy service&lt;/strong&gt; (Reliable solution)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deploy open-source API aggregation/proxy tools like OneAPI&lt;/li&gt;
&lt;li&gt;Configure as custom API endpoint in settings&lt;/li&gt;
&lt;li&gt;Request flow: Browser → Proxy service → Model service provider&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;q4-what-are-the-drawbacks-or-risks-of-using-vercel-proxy&#34;&gt;Q4: What are the drawbacks or risks of using Vercel proxy?
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Using Vercel proxy may trigger risk control mechanisms of some model service providers. Some vendors may identify requests from Vercel as proxy behavior, thereby limiting or denying service. If you encounter this issue, we recommend using a self-deployed proxy service.&lt;/p&gt;
&lt;h2 id=&#34;-contributing&#34;&gt;🤝 Contributing
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Fork the repository&lt;/li&gt;
&lt;li&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Commit your changes (&lt;code&gt;git commit -m &#39;Add some feature&#39;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Open a Pull Request&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Tip: When developing with Cursor tool, it is recommended to do the following before committing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use the &amp;ldquo;CodeReview&amp;rdquo; rule for review&lt;/li&gt;
&lt;li&gt;Check according to the review report format:
&lt;ul&gt;
&lt;li&gt;Overall consistency of changes&lt;/li&gt;
&lt;li&gt;Code quality and implementation method&lt;/li&gt;
&lt;li&gt;Test coverage&lt;/li&gt;
&lt;li&gt;Documentation completeness&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimize based on review results before submitting&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;-contributors&#34;&gt;👏 Contributors
&lt;/h2&gt;&lt;p&gt;Thanks to all the developers who have contributed to this project!&lt;/p&gt;
&lt;a href=&#34;https://github.com/linshenkx/prompt-optimizer/graphs/contributors&#34;&gt;
  &lt;img src=&#34;https://contrib.rocks/image?repo=linshenkx/prompt-optimizer&#34; alt=&#34;Contributors&#34; /&gt;
&lt;/a&gt;
&lt;h2 id=&#34;-license&#34;&gt;📄 License
&lt;/h2&gt;&lt;p&gt;This project is licensed under the &lt;a class=&#34;link&#34; href=&#34;https://github.com/linshenkx/prompt-optimizer/blob/master/LICENSE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MIT&lt;/a&gt; License.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If this project is helpful to you, please consider giving it a Star ⭐️&lt;/p&gt;
&lt;h2 id=&#34;-contact-us&#34;&gt;👥 Contact Us
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Submit an Issue&lt;/li&gt;
&lt;li&gt;Create a Pull Request&lt;/li&gt;
&lt;li&gt;Join the discussion group&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
